2025-m-13 13:10:04 [INFO] __main__ - Run Name: ModelBasedSAC_HIGH_DELAY_figure_8_20251113_131004
2025-m-13 13:10:04 [INFO] __main__ - Output Directory: ./rl_training_output/ModelBasedSAC_HIGH_DELAY_figure_8_20251113_131004
2025-m-13 13:10:04 [INFO] __main__ - Training Configuration:
2025-m-13 13:10:04 [INFO] __main__ -   Delay Config: HIGH_DELAY
2025-m-13 13:10:04 [INFO] __main__ -   Trajectory Type: figure_8
2025-m-13 13:10:04 [INFO] __main__ -   Randomize Trajectory: False
2025-m-13 13:10:04 [INFO] __main__ -   Total Timesteps: 3,000,000
2025-m-13 13:10:04 [INFO] __main__ -   Random Seed: 50
2025-m-13 13:10:04 [INFO] __main__ -   Pre-trained LSTM: /media/kai/Kai_Backup/Master_Study/Master_Thesis/Implementation/libfranka_ws/src/Model_based_Reinforcement_Learning_In_Teleoperation/Model_based_Reinforcement_Learning_In_Teleoperation/rl_agent/lstm_training_output/Pretrain_LSTM_HIGH_DELAY_20251113_082812/estimator_best.pth
2025-m-13 13:10:04 [INFO] __main__ - 
2025-m-13 13:10:04 [INFO] __main__ - Setting random seed: 50
2025-m-13 13:10:04 [INFO] __main__ - Random seeds set for NumPy, PyTorch, and CUDA
2025-m-13 13:10:04 [INFO] __main__ - 
2025-m-13 13:10:10 [INFO] __main__ -   Vectorized Environment: SubprocVecEnv
2025-m-13 13:10:10 [INFO] __main__ -   Number of Envs: 5
2025-m-13 13:10:10 [INFO] __main__ -   Observation Space (Env): (134,)
2025-m-13 13:10:10 [INFO] __main__ -   Action Space (Env): (7,)
2025-m-13 13:10:10 [INFO] __main__ - 
2025-m-13 13:10:10 [INFO] __main__ - Initializing Model-Based SAC trainer...
2025-m-13 13:10:13 [INFO] __main__ -   Trainer: SACTrainer
2025-m-13 13:10:13 [INFO] __main__ -   Checkpoint Directory: ./rl_training_output/ModelBasedSAC_HIGH_DELAY_figure_8_20251113_131004
2025-m-13 13:10:13 [INFO] __main__ - 
2025-m-13 13:10:13 [INFO] __main__ - ======================================================================
2025-m-13 13:10:13 [INFO] __main__ - Starting Training
2025-m-13 13:10:13 [INFO] __main__ - ======================================================================
2025-m-13 13:10:13 [INFO] __main__ - 
2025-m-13 13:10:13 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Tensorboard logs at: ./rl_training_output/ModelBasedSAC_HIGH_DELAY_figure_8_20251113_131004/tensorboard_sac
2025-m-13 13:10:13 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Starting training...
2025-m-13 13:10:51 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - 
──────────────────────────────────────────────────────────────────────
2025-m-13 13:10:51 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Timesteps: 5,000 | Updates: 5 | Elapsed: 0:00:37
2025-m-13 13:10:51 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Avg Reward (last 100): -11.130
2025-m-13 13:10:51 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Estimator Loss (Frozen): 0.196821
2025-m-13 13:10:51 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Actor Loss: -1.5375 | Critic Loss: 154.3236
2025-m-13 13:10:51 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Alpha: 0.9985 | Alpha Loss: -0.0141
2025-m-13 14:36:22 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - 
──────────────────────────────────────────────────────────────────────
2025-m-13 14:36:22 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Timesteps: 10,000 | Updates: 5,005 | Elapsed: 1:26:09
2025-m-13 14:36:22 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Avg Reward (last 100): -47.383
2025-m-13 14:36:22 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Estimator Loss (Frozen): 0.114947
2025-m-13 14:36:22 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Actor Loss: -0.6985 | Critic Loss: 0.1894
2025-m-13 14:36:22 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Alpha: 0.2424 | Alpha Loss: -7.6114
2025-m-13 15:57:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - 
──────────────────────────────────────────────────────────────────────
2025-m-13 15:57:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Timesteps: 15,000 | Updates: 10,005 | Elapsed: 2:47:26
2025-m-13 15:57:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Avg Reward (last 100): -58.962
2025-m-13 15:57:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Estimator Loss (Frozen): 0.111790
2025-m-13 15:57:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Actor Loss: -43.1941 | Critic Loss: 21.8330
2025-m-13 15:57:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Alpha: 0.1501 | Alpha Loss: 4.3246
2025-m-13 17:18:12 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - 
──────────────────────────────────────────────────────────────────────
2025-m-13 17:18:12 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Timesteps: 20,000 | Updates: 15,005 | Elapsed: 4:07:58
2025-m-13 17:18:12 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Avg Reward (last 100): -72.821
2025-m-13 17:18:12 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Estimator Loss (Frozen): 0.088893
2025-m-13 17:18:12 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Actor Loss: -70.1808 | Critic Loss: 13.9450
2025-m-13 17:18:12 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Alpha: 0.1592 | Alpha Loss: 1.2149
2025-m-13 17:40:12 [WARNING] __main__ - 
2025-m-13 17:40:12 [WARNING] __main__ - ======================================================================
2025-m-13 17:40:12 [WARNING] __main__ - Training Interrupted by User
2025-m-13 17:40:12 [WARNING] __main__ - ======================================================================
2025-m-13 17:40:12 [WARNING] __main__ - Saving interrupted model...
2025-m-13 17:40:12 [ERROR] __main__ - Could not save interrupted model: Parent directory ./rl_training_output/ModelBasedSAC_HIGH_DELAY_figure_8_20251113_131004/./rl_training_output/ModelBasedSAC_HIGH_DELAY_figure_8_20251113_131004 does not exist.
2025-m-13 17:40:12 [INFO] __main__ - 
2025-m-13 17:40:12 [INFO] __main__ - Cleaning up...
