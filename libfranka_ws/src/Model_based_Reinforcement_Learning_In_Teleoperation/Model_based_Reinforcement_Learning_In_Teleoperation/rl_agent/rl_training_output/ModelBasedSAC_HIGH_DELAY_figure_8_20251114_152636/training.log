2025-m-14 15:26:36 [INFO] __main__ - Training Configuration:
2025-m-14 15:26:36 [INFO] __main__ -   Delay Config: HIGH_DELAY
2025-m-14 15:26:36 [INFO] __main__ -   Trajectory Type: figure_8
2025-m-14 15:26:36 [INFO] __main__ -   Randomize Trajectory: False
2025-m-14 15:26:36 [INFO] __main__ -   Total Timesteps: 3,000,000
2025-m-14 15:26:36 [INFO] __main__ -   Random Seed: 50
2025-m-14 15:26:36 [INFO] __main__ - 
2025-m-14 15:26:36 [INFO] __main__ - Creating vectorized environment...
2025-m-14 15:26:38 [INFO] __main__ - Observation Structure (112D):
2025-m-14 15:26:38 [INFO] __main__ -   - Remote state: 14D (position 7D + velocity 7D)
2025-m-14 15:26:38 [INFO] __main__ -   - Remote history: 70D (5 timesteps × 14D)
2025-m-14 15:26:38 [INFO] __main__ -   - LSTM prediction: 14D (position 7D + velocity 7D)
2025-m-14 15:26:38 [INFO] __main__ -   - Current error: 14D (position error 7D + velocity error 7D)
2025-m-14 15:26:38 [INFO] __main__ -   - Total: 112D
2025-m-14 15:26:38 [INFO] __main__ - 
2025-m-14 15:26:38 [INFO] __main__ - Initializing Model-Based SAC trainer...
2025-m-14 15:26:39 [INFO] __main__ - ======================================================================
2025-m-14 15:26:39 [INFO] __main__ - Starting Training
2025-m-14 15:26:39 [INFO] __main__ - ======================================================================
2025-m-14 15:26:39 [INFO] __main__ - 
2025-m-14 15:26:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Tensorboard logs at: ./rl_training_output/ModelBasedSAC_HIGH_DELAY_figure_8_20251114_152636/tensorboard_sac
2025-m-14 15:26:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - ======================================================================
2025-m-14 15:26:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Starting SAC Training
2025-m-14 15:26:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - ======================================================================
2025-m-14 15:26:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Configuration:
2025-m-14 15:26:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Environments: 10
2025-m-14 15:26:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Total timesteps: 3,000,000
2025-m-14 15:26:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Learning rate (Actor/Critic): 0.0001
2025-m-14 15:26:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Gamma (discount): 0.9
2025-m-14 15:26:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Tau (soft update): 0.005
2025-m-14 15:26:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Batch size: 256
2025-m-14 15:26:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Buffer size: 1,000,000
2025-m-14 15:26:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Start steps (random): 5,000
2025-m-14 15:26:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Device: cuda
2025-m-14 15:26:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Observation dim: 112D (112: current 14D + history 70D + pred 14D + error 14D)
2025-m-14 15:26:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Actor input dim: 28D (predicted state 14D + remote state 14D)
2025-m-14 15:26:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - 
2025-m-14 15:26:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Training loop starting...
2025-m-14 15:26:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - 
2025-m-14 15:34:10 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - 
──────────────────────────────────────────────────────────────────────
2025-m-14 15:34:10 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Training Progress:
2025-m-14 15:34:10 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Timesteps: 10,000 / 3,000,000
2025-m-14 15:34:10 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Updates: 5,010
2025-m-14 15:34:10 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Elapsed Time: 0:07:30
2025-m-14 15:34:10 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Avg Episode Reward (train): -230.0710
2025-m-14 15:34:10 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - 
Environment Stats (avg over last 1000 steps):
2025-m-14 15:34:10 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Avg Real-Time Error (q): 74.508 mm
2025-m-14 15:34:10 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Avg Prediction Error (LSTM): 17.484 mm
2025-m-14 15:34:10 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Avg Delay (steps): 109.89
2025-m-14 15:34:10 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - 
Losses & Metrics:
2025-m-14 15:34:10 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Actor Loss: -13.0502
2025-m-14 15:34:10 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Critic Loss: 0.6448
2025-m-14 15:34:10 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Alpha: 0.2216
2025-m-14 15:34:10 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - ──────────────────────────────────────────────────────────────────────
2025-m-14 15:48:59 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - 
──────────────────────────────────────────────────────────────────────
2025-m-14 15:48:59 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Training Progress:
2025-m-14 15:48:59 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Timesteps: 20,000 / 3,000,000
2025-m-14 15:48:59 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Updates: 15,010
2025-m-14 15:48:59 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Elapsed Time: 0:22:19
2025-m-14 15:48:59 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Avg Episode Reward (train): -236.0979
2025-m-14 15:48:59 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - 
Environment Stats (avg over last 1000 steps):
2025-m-14 15:48:59 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Avg Real-Time Error (q): 79.217 mm
2025-m-14 15:48:59 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Avg Prediction Error (LSTM): 11.516 mm
2025-m-14 15:48:59 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Avg Delay (steps): 110.15
2025-m-14 15:48:59 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - 
Losses & Metrics:
2025-m-14 15:48:59 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Actor Loss: 1.0987
2025-m-14 15:48:59 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Critic Loss: 0.0904
2025-m-14 15:48:59 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Alpha: 0.0122
2025-m-14 15:48:59 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - ──────────────────────────────────────────────────────────────────────
2025-m-14 15:54:36 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 1/10: Reward = -244.0000
2025-m-14 15:54:38 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 2/10: Reward = -244.0000
2025-m-14 15:54:40 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 3/10: Reward = -244.0000
2025-m-14 15:54:42 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 4/10: Reward = -244.0000
2025-m-14 15:54:45 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 5/10: Reward = -244.0000
2025-m-14 15:54:47 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 6/10: Reward = -244.0000
2025-m-14 15:54:49 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 7/10: Reward = -244.0000
2025-m-14 15:54:52 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 8/10: Reward = -244.0000
2025-m-14 15:54:54 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 9/10: Reward = -244.0000
2025-m-14 15:54:56 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 10/10: Reward = -244.0000
2025-m-14 15:54:56 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - 
Validation Results:
2025-m-14 15:54:56 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Average Reward: -244.0000
2025-m-14 15:54:56 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Std Dev: 0.0000
2025-m-14 15:54:56 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Min: -244.0000
2025-m-14 15:54:56 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Max: -244.0000
2025-m-14 15:54:56 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Checkpoint saved: ./rl_training_output/ModelBasedSAC_HIGH_DELAY_figure_8_20251114_152636/best_policy.pth
2025-m-14 15:54:56 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - ✓ NEW BEST! Validation reward: -244.0000
2025-m-14 15:54:56 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Saved: best_policy.pth
2025-m-14 15:58:45 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - 
──────────────────────────────────────────────────────────────────────
2025-m-14 15:58:45 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Training Progress:
2025-m-14 15:58:45 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Timesteps: 30,000 / 3,000,000
2025-m-14 15:58:45 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Updates: 25,010
2025-m-14 15:58:45 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Elapsed Time: 0:32:05
2025-m-14 15:58:45 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Avg Episode Reward (train): -238.6625
2025-m-14 15:58:45 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - 
Environment Stats (avg over last 1000 steps):
2025-m-14 15:58:45 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Avg Real-Time Error (q): 86.929 mm
2025-m-14 15:58:45 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Avg Prediction Error (LSTM): 11.479 mm
2025-m-14 15:58:45 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Avg Delay (steps): 110.09
2025-m-14 15:58:45 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - 
Losses & Metrics:
2025-m-14 15:58:45 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Actor Loss: 1.1029
2025-m-14 15:58:45 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Critic Loss: 0.0310
2025-m-14 15:58:45 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Alpha: 0.0079
2025-m-14 15:58:45 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - ──────────────────────────────────────────────────────────────────────
2025-m-14 16:03:26 [WARNING] __main__ - 
2025-m-14 16:03:26 [WARNING] __main__ - ======================================================================
2025-m-14 16:03:26 [WARNING] __main__ - Training Interrupted by User
2025-m-14 16:03:26 [WARNING] __main__ - ======================================================================
2025-m-14 16:03:26 [WARNING] __main__ - Saving interrupted model...
2025-m-14 16:03:26 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Checkpoint saved: ./rl_training_output/ModelBasedSAC_HIGH_DELAY_figure_8_20251114_152636/interrupted_policy.pth
2025-m-14 16:03:26 [INFO] __main__ - Interrupted model saved to: ./rl_training_output/ModelBasedSAC_HIGH_DELAY_figure_8_20251114_152636
2025-m-14 16:03:26 [INFO] __main__ - 
2025-m-14 16:03:26 [INFO] __main__ - Cleaning up...
2025-m-14 16:03:26 [INFO] __main__ - Environment closed
2025-m-14 16:03:26 [INFO] __main__ - 
2025-m-14 16:03:26 [INFO] __main__ - ======================================================================
2025-m-14 16:03:26 [INFO] __main__ - Output Directory: ./rl_training_output/ModelBasedSAC_HIGH_DELAY_figure_8_20251114_152636
2025-m-14 16:03:26 [INFO] __main__ - ======================================================================
2025-m-14 16:03:26 [INFO] __main__ - Training ended prematurely.
