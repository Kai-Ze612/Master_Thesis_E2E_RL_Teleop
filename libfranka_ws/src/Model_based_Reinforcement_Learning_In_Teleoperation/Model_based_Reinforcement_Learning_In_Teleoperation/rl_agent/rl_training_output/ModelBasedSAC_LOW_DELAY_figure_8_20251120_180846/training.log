2025-11-20 18:08:46 [INFO] __main__ - Training Configuration:
2025-11-20 18:08:46 [INFO] __main__ -   Delay Config: LOW_DELAY
2025-11-20 18:08:46 [INFO] __main__ -   Trajectory Type: figure_8
2025-11-20 18:08:46 [INFO] __main__ -   Randomize Trajectory: False
2025-11-20 18:08:46 [INFO] __main__ -   Total Timesteps: 3,000,000
2025-11-20 18:08:46 [INFO] __main__ -   Random Seed: None (random)
2025-11-20 18:08:46 [INFO] __main__ - 
2025-11-20 18:08:46 [INFO] __main__ - Creating vectorized environment...
2025-11-20 18:08:47 [INFO] __main__ - Observation Structure (112D):
2025-11-20 18:08:47 [INFO] __main__ -   - Remote state: 14D (position 7D + velocity 7D)
2025-11-20 18:08:47 [INFO] __main__ -   - Remote history: 70D (5 timesteps x 14D)
2025-11-20 18:08:47 [INFO] __main__ -   - LSTM prediction: 14D (position 7D + velocity 7D)
2025-11-20 18:08:47 [INFO] __main__ -   - Current error: 14D (position error 7D + velocity error 7D)
2025-11-20 18:08:47 [INFO] __main__ -   - Total: 113D
2025-11-20 18:08:47 [INFO] __main__ - 
2025-11-20 18:08:47 [INFO] __main__ - Initializing Model-Based SAC trainer...
2025-11-20 18:08:48 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Loaded estimator from /home/kaize/Downloads/Master_Study_Master_Thesis/libfranka_ws/src/Model_based_Reinforcement_Learning_In_Teleoperation/Model_based_Reinforcement_Learning_In_Teleoperation/rl_agent/lstm_training_output/Low_Delay_No_Rand/estimator_best.pth
2025-11-20 18:08:48 [INFO] __main__ - ======================================================================
2025-11-20 18:08:48 [INFO] __main__ - Starting Training
2025-11-20 18:08:48 [INFO] __main__ - ======================================================================
2025-11-20 18:08:48 [INFO] __main__ - 
2025-11-20 18:08:48 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Tensorboard logs at: ./rl_training_output/ModelBasedSAC_LOW_DELAY_figure_8_20251120_180846/tensorboard_sac
2025-11-20 18:08:48 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - ======================================================================
2025-11-20 18:08:48 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Starting SAC Training
2025-11-20 18:08:48 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - ======================================================================
2025-11-20 18:08:48 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Configuration:
2025-11-20 18:08:48 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Environments: 1
2025-11-20 18:08:48 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Total timesteps: 3,000,000
2025-11-20 18:08:48 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Learning rate (Actor/Critic): 0.0003
2025-11-20 18:08:48 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Gamma (discount): 0.99
2025-11-20 18:08:48 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Tau (soft update): 0.005
2025-11-20 18:08:48 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Batch size: 256
2025-11-20 18:08:48 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Buffer size: 1,000,000
2025-11-20 18:08:48 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Start steps (random): 20,000
2025-11-20 18:08:48 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Device: cuda
2025-11-20 18:08:48 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Observation dim: 113D
2025-11-20 18:08:48 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Actor input dim: 28D (predicted state 14D + remote state 14D)
2025-11-20 18:08:48 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - 
2025-11-20 18:08:48 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Training loop starting...
2025-11-20 18:08:48 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - 
2025-11-20 18:08:49 [ERROR] __main__ - 
2025-11-20 18:08:49 [ERROR] __main__ - ======================================================================
2025-11-20 18:08:49 [ERROR] __main__ - Training Failed with Error
2025-11-20 18:08:49 [ERROR] __main__ - ======================================================================
2025-11-20 18:08:49 [ERROR] __main__ - Error: could not broadcast input array from shape (14,) into shape (15,)
Traceback (most recent call last):
  File "/home/kaize/Downloads/Master_Study_Master_Thesis/libfranka_ws/src/Model_based_Reinforcement_Learning_In_Teleoperation/Model_based_Reinforcement_Learning_In_Teleoperation/rl_agent/train_agent.py", line 207, in train_agent
    trainer.train(total_timesteps=args.timesteps)
  File "/home/kaize/Downloads/Master_Study_Master_Thesis/libfranka_ws/src/Model_based_Reinforcement_Learning_In_Teleoperation/Model_based_Reinforcement_Learning_In_Teleoperation/rl_agent/sac_training_algorithm.py", line 501, in train
    self.replay_buffer.add(
  File "/home/kaize/Downloads/Master_Study_Master_Thesis/libfranka_ws/src/Model_based_Reinforcement_Learning_In_Teleoperation/Model_based_Reinforcement_Learning_In_Teleoperation/rl_agent/sac_training_algorithm.py", line 86, in add
    self.true_targets[self.ptr] = true_target
    ~~~~~~~~~~~~~~~~~^^^^^^^^^^
ValueError: could not broadcast input array from shape (14,) into shape (15,)
2025-11-20 18:08:49 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Checkpoint saved: ./rl_training_output/ModelBasedSAC_LOW_DELAY_figure_8_20251120_180846/crash_policy.pth
2025-11-20 18:08:49 [INFO] __main__ - Crash model saved to: ./rl_training_output/ModelBasedSAC_LOW_DELAY_figure_8_20251120_180846
2025-11-20 18:08:49 [INFO] __main__ - 
2025-11-20 18:08:49 [INFO] __main__ - Cleaning up...
2025-11-20 18:08:49 [INFO] __main__ - Environment closed
2025-11-20 18:08:49 [INFO] __main__ - 
2025-11-20 18:08:49 [INFO] __main__ - ======================================================================
2025-11-20 18:08:49 [INFO] __main__ - Output Directory: ./rl_training_output/ModelBasedSAC_LOW_DELAY_figure_8_20251120_180846
2025-11-20 18:08:49 [INFO] __main__ - ======================================================================
2025-11-20 18:08:49 [INFO] __main__ - Training ended prematurely.
