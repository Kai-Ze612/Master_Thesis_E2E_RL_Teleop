2025-11-21 10:39:34 [INFO] __main__ - Training Configuration:
2025-11-21 10:39:34 [INFO] __main__ -   Delay Config: LOW_DELAY
2025-11-21 10:39:34 [INFO] __main__ -   Trajectory Type: figure_8
2025-11-21 10:39:34 [INFO] __main__ -   Randomize Trajectory: False
2025-11-21 10:39:34 [INFO] __main__ -   Total Timesteps: 3,000,000
2025-11-21 10:39:34 [INFO] __main__ -   Random Seed: None (random)
2025-11-21 10:39:34 [INFO] __main__ - 
2025-11-21 10:39:34 [INFO] __main__ - Creating vectorized environment...
2025-11-21 10:39:37 [INFO] __main__ - Observation Structure (112D):
2025-11-21 10:39:37 [INFO] __main__ -   - Remote state: 14D (position 7D + velocity 7D)
2025-11-21 10:39:37 [INFO] __main__ -   - Remote history: 70D (5 timesteps x 14D)
2025-11-21 10:39:37 [INFO] __main__ -   - LSTM prediction: 14D (position 7D + velocity 7D)
2025-11-21 10:39:37 [INFO] __main__ -   - Current error: 14D (position error 7D + velocity error 7D)
2025-11-21 10:39:37 [INFO] __main__ -   - Total: 113D
2025-11-21 10:39:37 [INFO] __main__ - 
2025-11-21 10:39:37 [INFO] __main__ - Initializing Model-Based SAC trainer...
2025-11-21 10:39:38 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Loaded estimator from /home/kaize/Downloads/Master_Study_Master_Thesis/libfranka_ws/src/Model_based_Reinforcement_Learning_In_Teleoperation/Model_based_Reinforcement_Learning_In_Teleoperation/rl_agent/lstm_training_output/Low_Delay_No_Rand/estimator_best.pth
2025-11-21 10:39:38 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Initializing Actor and Critic networks with state_dim = 29 (predicted_state 14D + remote_augmented_state 15D)
2025-11-21 10:39:39 [INFO] __main__ - ======================================================================
2025-11-21 10:39:39 [INFO] __main__ - Starting Training
2025-11-21 10:39:39 [INFO] __main__ - ======================================================================
2025-11-21 10:39:39 [INFO] __main__ - 
2025-11-21 10:39:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Tensorboard logs at: ./rl_training_output/ModelBasedSAC_LOW_DELAY_figure_8_20251121_103934/tensorboard_sac
2025-11-21 10:39:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - ======================================================================
2025-11-21 10:39:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Starting SAC Training
2025-11-21 10:39:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - ======================================================================
2025-11-21 10:39:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Configuration:
2025-11-21 10:39:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Environments: 20
2025-11-21 10:39:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Total timesteps: 3,000,000
2025-11-21 10:39:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Learning rate (Actor/Critic): 0.0003
2025-11-21 10:39:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Gamma (discount): 0.99
2025-11-21 10:39:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Tau (soft update): 0.005
2025-11-21 10:39:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Batch size: 256
2025-11-21 10:39:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Buffer size: 1,000,000
2025-11-21 10:39:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Start steps (random): 20,000
2025-11-21 10:39:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Device: cuda
2025-11-21 10:39:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Observation dim: 113D
2025-11-21 10:39:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Actor input dim: 28D (predicted state 14D + remote state 14D)
2025-11-21 10:39:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - 
2025-11-21 10:39:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Training loop starting...
2025-11-21 10:39:39 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - 
2025-11-21 10:39:50 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - 
──────────────────────────────────────────────────────────────────────
2025-11-21 10:39:50 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Timesteps: 20,000 | Updates: 20
2025-11-21 10:39:50 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Elapsed Time: 0:00:11
2025-11-21 10:39:50 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Avg Reward (last 100 ep): 0.0000
2025-11-21 10:39:50 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - 
Environment Stats (avg over last 1000 steps):
2025-11-21 10:39:50 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -  Avg Real-Time Error (q): 0.4368 rad
2025-11-21 10:39:50 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -  Avg Prediction Error (LSTM): 0.1423 rad
2025-11-21 10:39:50 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -  Avg Delay (steps): 6.04
2025-11-21 10:40:40 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 1/10: Reward = -2787.7609
2025-11-21 10:40:40 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 2/10: Reward = -2787.7036
2025-11-21 10:40:40 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 3/10: Reward = -2787.6487
2025-11-21 10:40:40 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 4/10: Reward = -2787.5957
2025-11-21 10:40:40 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 5/10: Reward = -2787.5450
2025-11-21 10:40:40 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 6/10: Reward = -2787.4960
2025-11-21 10:40:40 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 7/10: Reward = -2787.4495
2025-11-21 10:40:40 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 8/10: Reward = -2787.4046
2025-11-21 10:40:40 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 9/10: Reward = -2787.3617
2025-11-21 10:40:40 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 10/10: Reward = -2787.3203
2025-11-21 10:40:40 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - 
Validation Results:
2025-11-21 10:40:40 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Average Reward: -2787.5286
2025-11-21 10:40:40 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Std Dev: 0.1406
2025-11-21 10:40:40 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Min: -2787.7609
2025-11-21 10:40:40 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Max: -2787.3203
2025-11-21 10:40:40 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Checkpoint saved: ./rl_training_output/ModelBasedSAC_LOW_DELAY_figure_8_20251121_103934/best_policy.pth
2025-11-21 10:40:40 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - ✓ NEW BEST! Validation reward: -2787.5286
2025-11-21 10:40:40 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Saved: best_policy.pth
2025-11-21 10:43:10 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - 
──────────────────────────────────────────────────────────────────────
2025-11-21 10:43:10 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Timesteps: 40,000 | Updates: 20,020
2025-11-21 10:43:10 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Elapsed Time: 0:03:31
2025-11-21 10:43:10 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Avg Reward (last 100 ep): 0.0000
2025-11-21 10:43:10 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - 
Environment Stats (avg over last 1000 steps):
2025-11-21 10:43:10 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -  Avg Real-Time Error (q): 0.4732 rad
2025-11-21 10:43:10 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -  Avg Prediction Error (LSTM): 0.1412 rad
2025-11-21 10:43:10 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -  Avg Delay (steps): 6.00
2025-11-21 10:46:32 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 1/10: Reward = -2787.2423
2025-11-21 10:46:32 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 2/10: Reward = -2787.2058
2025-11-21 10:46:32 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 3/10: Reward = -2787.1709
2025-11-21 10:46:32 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 4/10: Reward = -2787.1372
2025-11-21 10:46:32 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 5/10: Reward = -2787.1051
2025-11-21 10:46:32 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 6/10: Reward = -2787.0742
2025-11-21 10:46:32 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 7/10: Reward = -2787.0441
2025-11-21 10:46:32 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 8/10: Reward = -2787.0157
2025-11-21 10:46:32 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 9/10: Reward = -2786.9883
2025-11-21 10:46:32 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Episode 10/10: Reward = -2786.9621
2025-11-21 10:46:32 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - 
Validation Results:
2025-11-21 10:46:32 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Average Reward: -2787.0946
2025-11-21 10:46:32 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Std Dev: 0.0894
2025-11-21 10:46:32 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Min: -2787.2423
2025-11-21 10:46:32 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Max: -2786.9621
2025-11-21 10:46:32 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Checkpoint saved: ./rl_training_output/ModelBasedSAC_LOW_DELAY_figure_8_20251121_103934/best_policy.pth
2025-11-21 10:46:32 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - ✓ NEW BEST! Validation reward: -2787.0946
2025-11-21 10:46:32 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm -   Saved: best_policy.pth
2025-11-21 10:48:41 [WARNING] __main__ - 
2025-11-21 10:48:41 [WARNING] __main__ - ======================================================================
2025-11-21 10:48:41 [WARNING] __main__ - Training Interrupted by User
2025-11-21 10:48:41 [WARNING] __main__ - ======================================================================
2025-11-21 10:48:41 [WARNING] __main__ - Saving interrupted model...
2025-11-21 10:48:41 [INFO] Model_based_Reinforcement_Learning_In_Teleoperation.rl_agent.sac_training_algorithm - Checkpoint saved: ./rl_training_output/ModelBasedSAC_LOW_DELAY_figure_8_20251121_103934/interrupted_policy.pth
2025-11-21 10:48:41 [INFO] __main__ - Interrupted model saved to: ./rl_training_output/ModelBasedSAC_LOW_DELAY_figure_8_20251121_103934
2025-11-21 10:48:41 [INFO] __main__ - 
2025-11-21 10:48:41 [INFO] __main__ - Cleaning up...
2025-11-21 10:48:41 [INFO] __main__ - Environment closed
2025-11-21 10:48:41 [INFO] __main__ - 
2025-11-21 10:48:41 [INFO] __main__ - ======================================================================
2025-11-21 10:48:41 [INFO] __main__ - Output Directory: ./rl_training_output/ModelBasedSAC_LOW_DELAY_figure_8_20251121_103934
2025-11-21 10:48:41 [INFO] __main__ - ======================================================================
2025-11-21 10:48:41 [INFO] __main__ - Training ended prematurely.
